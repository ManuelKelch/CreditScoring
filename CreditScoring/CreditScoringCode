import numpy as np
import sklearn as sk
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPClassifier


def get_optimal_params_for_clf(samples,labels):
    # scale samples
    mlp = MLPClassifier(max_iter=10000)
    samples_scaled = sk.preprocessing.scale(samples)
    print(samples_scaled)
    params = {
        'hidden_layer_sizes': [(9,), (21,), (30,)],
        'activation': ['tanh', 'relu', 'logistic'],
        'solver': ['sgd', 'adam'],
    }
    # split up data for test and training
    X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(samples_scaled, labels, test_size=0.3333, random_state=1)

    clf = GridSearchCV(mlp, params, n_jobs=-1, cv=5)
    clf.fit(X_train, y_train)
    # Best parameter set
    print('Best parameters found:\n', clf.best_params_,"%0.3f"% clf.best_score_)

    # All results
    means = clf.cv_results_['mean_test_score']
    stds = clf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

# check if preprocessing is necessary with decision tree
def preprocessing(samples,labels,clf):
    samples_std = sk.preprocessing.scale(samples)
    samples_norm = sk.preprocessing.normalize(samples)

    x_train, x_test, y_train, y_test = sk.model_selection.train_test_split(samples, labels, test_size=0.3333, random_state=1)
    x_train_std, x_test_std, y_train_std, y_test_std = sk.model_selection.train_test_split(samples_std, labels, test_size=0.3333, random_state=1)
    x_train_norm, x_test_norm, y_train_norm, y_test_norm = sk.model_selection.train_test_split(samples_norm, labels, test_size=0.3333, random_state=1)

    clf.fit(x_train,y_train)
    pred = clf.predict(x_test)
    clf.fit(x_train_std,y_train_std)
    pred_std = clf.predict(x_test_std)
    clf.fit(x_train_norm,y_train_norm)
    pred_norm = clf.predict(x_test_norm)

    scores1 = cross_val_score(clf, samples, labels, cv=20, scoring='accuracy')
    scores2 = cross_val_score(clf, samples, labels, cv=20, scoring='precision')

    scores1_std = cross_val_score(clf, samples_std, labels, cv=20, scoring='accuracy')
    scores2_std = cross_val_score(clf, samples_std, labels, cv=20, scoring='precision')

    scores1_norm = cross_val_score(clf, samples_norm, labels, cv=20, scoring='accuracy')
    scores2_norm = cross_val_score(clf, samples_norm, labels, cv=20, scoring='precision')

    print('CV Accuracy normal: ' + "{:.9f}".format(scores1.mean()))
    print('CV Precision normal: ' + "{:.9f}".format(scores2.mean()))
    print('accuracy score normal: ' + "{:.9f}".format(sk.metrics.accuracy_score(y_test,pred)))
    print('CV Accuracy standardized: ' + "{:.9f}".format(scores1_std.mean()))
    print('CV Precision standardized: ' + "{:.9f}".format(scores2_std.mean()))
    print('accuracy score standardized: ' + "{:.9f}".format(sk.metrics.accuracy_score(y_test_std, pred_std)))
    print('CV Accuracy normalized: ' + "{:.9f}".format(scores1_norm.mean()))
    print('CV Precision normalized: ' + "{:.9f}".format(scores2_norm.mean()))
    print('accuracy score normal: ' + "{:.9f}".format(sk.metrics.accuracy_score(y_test_norm, pred_norm)))


def testNeuralNetwork():
    #create best 2 found parameterized neural networks and test them for their accuracy score
    mlp = MLPClassifier(max_iter=10000, hidden_layer_sizes=(30,), activation="relu", solver='sgd')
    mlp1 = MLPClassifier(max_iter=10000, shuffle=True, hidden_layer_sizes=(9,), activation='tanh', solver='sgd')
    X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(samples_scaled, labels, test_size=0.3333,
                                                                           random_state=1)
    mlp.fit(X_train, y_train)
    mlp1.fit(X_train,y_train)
    predictions = mlp.predict(X_test)
    predictions1 = mlp1.predict(X_test)
    score = sk.metrics.accuracy_score(y_test, predictions)
    score1 = sk.metrics.accuracy_score(y_test, predictions1)
    print(score)
    print(score1)


def baggingEstimator(base_classifier):
    #create bagging classifier for parameter optimization
    bclf = BaggingClassifier(base_estimator=base_classifier)
    #create classifier which was found during parameter optimization
    bclf1 = BaggingClassifier(base_estimator=base_classifier,n_estimators=18,max_samples=0.5, max_features=1.0)
    #split dataset into training and test data 66,66% training 33,33%testing
    x_train, x_test, y_train, y_test = sk.model_selection.train_test_split(samples_scaled, labels, test_size=0.3333, random_state=1)
    # train classifier and predict test samples, print accuracy
    bclf1.fit(x_train,y_train)
    predictions = bclf1.predict(x_test)
    print(sk.metrics.accuracy_score(y_test,predictions))
    #set params for parameter optimization
    params = {
        'n_estimators': [18,20,25,30],
        'max_samples': [1.0, 0.5, 0.7],
        'max_features': [1.0, 0.5, 0.7],
    }
    # train GridSearchCV with different parameter options and get best result
    clf = GridSearchCV(bclf, params, n_jobs=-1, cv=5)
    clf.fit(x_train, y_train)
    # Best parameter set
    print('Best parameters found:\n', clf.best_params_,"%0.3f"% clf.best_score_)

    # All results
    means = clf.cv_results_['mean_test_score']
    stds = clf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))


# read in csv with dataset and split it to samples and labels
dataset = np.genfromtxt('german_credit.csv', delimiter=',', skip_header = 1)
labels = dataset[:, 0]
samples = np.delete(dataset, 0, 1)
samples_scaled = sk.preprocessing.scale(samples)

# check if preprocessing is necessary
#preprocessing(samples,labels,dec_tree)
#check optimal params for classifier
#get_optimal_params_for_clf(samples,labels)
#test neural network
#testNeuralNetwork()
#check for parameter optimization for bagging classifier using neural network found in previous steps
mlp = MLPClassifier(max_iter=10000, shuffle=True, hidden_layer_sizes=(9,), activation='tanh', solver='sgd')
baggingEstimator(mlp)

